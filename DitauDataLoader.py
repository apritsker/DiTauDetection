"""
DitauDataLoader.py:
Handles loading the data generated by DitauDataGenerator.py
"""

from torch.utils.data import Dataset, DataLoader, Sampler
import torch
import numpy as np
import pickle
import glob
from tqdm import tqdm
import os
from train import SIMPLE_LABEL

class DiTauDataset(Dataset):
    def __init__(self, data_dir, transform=None):
        self.data_dir = data_dir

        with open(data_dir + '/normalization_data.pkl', 'rb') as fn:
            self.norm_data = pickle.load(fn)

        with open(data_dir + '/track_sizes.pkl', 'rb') as fs:
            self.track_sizes = pickle.load(fs)

        file_path = self.data_dir + '/DiTauDataset_data.pkl'
        self.max_em_size = -100000000
        self.max_had_size = -10000000
        if os.path.isfile(file_path):
            with open(file_path, 'rb') as fi:
                self.max_em_size, self.max_had_size, self.signals, self.backgrounds = pickle.load(fi)
        else:
            self.signals = 0
            self.backgrounds = 0
            for i in tqdm(range(len(glob.glob(self.data_dir + '/*')) - 3)):
                sample_file_path = self.data_dir + '/sample_' + str(i).zfill(6) + '.pkl'
                with open(sample_file_path, 'rb') as fi:
                    obj = pickle.load(fi)

                    if int(obj['label']):
                        self.signals += 1
                    else:
                        self.backgrounds += 1
                    self.max_em_size = max(self.max_em_size, np.array(obj['em']).shape[0])
            with open(file_path, 'wb') as fo:
                pickle.dump((self.max_em_size, self.max_had_size, self.signals, self.backgrounds), fo)
        self.max_tracks_padding = max(512, max(self.track_sizes))
        print('INFO:Dataset has: ' + str(self.signals) + ' signals, and ' + str(self.backgrounds) + ' background')

    def __len__(self):
        return len(glob.glob(self.data_dir + '/*')) - 3

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
            # print(idx)

        file_path = self.data_dir + '/sample_' + str(idx).zfill(6) + '.pkl'

        with open(file_path, 'rb') as fi:
            obj = pickle.load(fi)
            # return {'cell_image': obj['cell_image'], 'label': int(obj['label']), 'tracks': obj['tracks_arr']}

            tracks = torch.from_numpy(obj['tracks_arr'])
            tracks = torch.cat([tracks, torch.zeros((int(self.max_tracks_padding) - tracks.shape[0],3))])
            cell_image = obj['cell_image']
            #cell_image = np.zeros((3,50,50))
            #cell_image[0, :, :] = cell_image_temp[:, :, 0]
            #cell_image[1, :, :] = cell_image_temp[:, :, 1]
            #cell_image[2, :, :] = cell_image_temp[:, :, 2]
            em = obj['em']
            em = torch.from_numpy(np.array(em))
            em = torch.cat([em, torch.zeros((max(self.max_em_size, self.max_had_size)-em.shape[0], 3))]) # 200->max cells size

            had = obj['had']
            had = torch.from_numpy(np.array(had))
            had = torch.cat([had, torch.zeros((max(self.max_had_size, self.max_em_size) - had.shape[0], 3))])  # 200->max cells size

            #if not SIMPLE_LABEL:
            label = torch.zeros((2,))
            label[int(obj['label'])] = 1
            #else:
            #    label = int(obj['label'])
            cells = torch.zeros((20, 3))
        return tracks, em, had, torch.from_numpy(cell_image), label, {'event_num':obj['event_num'], 'sample_num':obj['sample_num']}
            

class BatchSampler(Sampler):
    def __init__(self, n_nodes_array, batch_size):
        # n_nodes_array[] == event_size[], batch_size = 128
        """
        Initialization
        :param n_nodes_array: array of sizes of the events
        """
        super().__init__(n_nodes_array.size)
        # print(n_nodes_array[:10])
        self.dataset_size = n_nodes_array.size
        self.batch_size = batch_size

        self.index_to_batch = {}
        self.node_size_idx = {}
        running_idx = -1
        for n_nodes_i in set(n_nodes_array):
            self.node_size_idx[n_nodes_i] = np.where(n_nodes_array == n_nodes_i)[0]

            n_of_size = len(self.node_size_idx[n_nodes_i])
            n_batches = max(n_of_size / self.batch_size, 1)

            self.node_size_idx[n_nodes_i] = np.array_split(np.random.permutation(self.node_size_idx[n_nodes_i]),
                                                           n_batches)
            for batch in self.node_size_idx[n_nodes_i]:
                running_idx += 1
                self.index_to_batch[running_idx] = batch

        self.n_batches = running_idx + 1

    def __len__(self):
        return self.n_batches

    def __iter__(self):
        batch_order = np.random.permutation(np.arange(self.n_batches))
        for i in batch_order:
            yield self.index_to_batch[i]


class SplitBatchSampler(Sampler):
    def __init__(self, n_nodes_array, batch_size, split_indices):
        # n_nodes_array[] == event_size[], batch_size = 128
        """
        Initialization
        :param n_nodes_array: array of sizes of the events
        """
        super().__init__(n_nodes_array.size)
        
        self.split_sizes = {} # split index -> track size.
        for idx in split_indices:
            self.split_sizes[idx] = n_nodes_array[idx]

        self.dataset_size = split_indices.size
        self.batch_size = batch_size

        self.index_to_batch = {}
        self.node_size_idx = {}
        running_idx = -1
        for n_nodes_i in set(self.split_sizes.values()):
            self.node_size_idx[n_nodes_i] = np.array([k for k,v in self.split_sizes.items() if v == n_nodes_i])

            n_of_size = len(self.node_size_idx[n_nodes_i])
            n_batches = max(n_of_size / self.batch_size, 1)

            self.node_size_idx[n_nodes_i] = np.array_split(np.random.permutation(self.node_size_idx[n_nodes_i]),
                                                           n_batches)
            for batch in self.node_size_idx[n_nodes_i]:
                running_idx += 1
                self.index_to_batch[running_idx] = batch

        self.n_batches = running_idx + 1

    def __len__(self):
        return self.n_batches

    def __iter__(self):
        batch_order = np.random.permutation(np.arange(self.n_batches))
        for i in batch_order:
            yield self.index_to_batch[i]